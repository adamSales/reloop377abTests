% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage[utf8]{inputenc}
<<librarysEtc,echo=FALSE,include=FALSE,cache=FALSE>>=
library(scales)
library(tidyverse)
library(kableExtra)
library(xtable)
library(knitr)
library(tikzDevice)

pn=function(x) prettyNum(x,big.mark=',')
@

<<options,include=FALSE>>=
opts_chunk$set(message=FALSE,warning=FALSE,error=FALSE,echo=FALSE,cache=TRUE,dev='tikz')
@

<<loadStuff,include=FALSE,cache=FALSE>>=
load('../results/analysisResults.RData')
#load('../results/plots.RData'
@ 

\input{notation.tex}

\begin{document}
\SweaveOpts{concordance=TRUE}
%
\title{More Powerful A/B Testing using Auxiliary Data and Deep Learning}
%
\titlerunning{A/B Tests with Auxiliary Data}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Adam C. Sales\inst{1}\orcidID{0000-1111-2222-3333} \and
Ethan Prihar\inst{1}\orcidID{1111-2222-3333-4444} \and
Johann Gagnon-Bartsch\inst{2}\orcidID{2222--3333-4444-5555}\and
Ashish Gurung\inst{1}\and
Neil T. Heffernan\inst{1}}
%
\authorrunning{A. C. Sales et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Worcester Polytechnic Institute, Worcester, MA 01609, USA \and
University of Michigan, Ann Arbor, MA 48109, USA}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Randomized A/B tests within online learning platforms represent an exciting direction in learning sciences. With minimal assumptions, they allow causal effect estimation without confounding bias and exact statistical inference even in small samples. However, often experimental samples and/or treatment effects are small, A/B tests are under-powered, and effect estimates are overly imprecise. Recent methodological advances have shown that power and statistical precision can be substantially boosted by coupling design-based causal estimation to machine-learning models of rich log data from historical users who were not in the experiment. Estimates using these techniques remain unbiased and inference remains exact without any additional assumptions. This paper reviews those methods and applies them to a new dataset including over 250 randomized A/B comparisons conducted within ASSISTments, an online learning platform. We compare results across experiments using four novel deep-learning models of auxiliary data, and show that incorporating auxiliary data into causal estimates is roughly equivalent to increasing the sample size by 20\% on average, or as much as 50-80\% in some cases, relative to t-tests, and by about 10\% on average, or as much as 30-50\%, compared to cutting-edge machine learning unbiased estimates that use only data from the experiments.

\keywords{A/B Tests  \and Deep Learning \and Evaluation}
\end{abstract}
%
%
%
\section{Introduction}
In randomized A/B tests on an online learning platform, students are randomized between different educational conditions or strategies, and their subsequent educational outcomes of interest are compared between different conditions.
For instance, \cite{harrison2020spacing} studied data from 2,152 middle- and high-school students whose teachers assigned a specific module---a ``skill builder''---on the ASSISTments online tutoring platform \cite{heffernan2014assistments}. Prior to the students' work, the authors designed four different educational conditions, which differed in how the numbers and symbols in arithmatic expressions were spaced. As students logged on to the platform, in the course of their usual schoolwork, they were each individually randomized to 
one of the four conditions, and completed their work under that condition. Subsequently, the authors of the study compared the average number of problems students in each condition had to work before acheiving mastery, defined as answering three problems correct in a row. They found that students who were assigned the the ``congruent'' condition---in which the spacing between numbers corresponded to the order of operations---needed to work on roughly one fewer problem, on average, than students in the ``incongruent'' condition. This finding, and others reported in the paper, validated their previous scientific hypotheses regarding embodied cognition, the relationship between abstract learning and the arrangment of objects in physical (or virtual) space.

In general A/B tests have two significant advantages over observational study designs, which do not include randomization, and additional advantages over studies conducted in a lab. First, they are (famously) free of confounding bias---since students are randomly allocated between conditions, differences in outcomes must be due to either a causal effect of the randomized conditions or to random error, but not to baseline differences between students, observed or unobserved. Perhaps less famously, randomization forms a ``reasoned basis for inference'' \cite{fisher1935design}: the (known) probabilities of allocation of students between experimental conditions provide nearly all of the necessary justification for the unbiased estimation of causal effects, as well as standard errors, confidence intervals, and p-values. No other distributional assumptions or modeling assumptions are necessary. These properties allowed \cite{harrison2020spacing} to estimate causal effects of spacing conditions, as well as to statistically rule out other alternative explanations.\footnote{Acutally the authors of that paper did make modeling assumptions in their analysis, but they could have conducted a non-parametric analysis.} Causal effect and standard error estimators that rely only on the experimental design are referred to as ``design-based'' \cite{schochet2015statistical}. 
%Compared to laboratory studies, A/B tests allow researchers to estimate causal parameters under real-life conditions, for representative (if not randomly sampled) groups of students.

On the other hand, A/B tests can be hobbled by statistical imprecision. For instance, \cite{harrison2020spacing} was unable to confirm or disconfirm one of their initial hypotheses, regarding differences in causal effects between subgroups of students, because the standard errors of the relevant estimates were too high. Unlike observational studies using data from online tutors, the sample size in A/B tests is necessarily limited to those students who worked on the relevant modules while the study was taking place. In contrast, an observational study can use data from all students who have ever worked on the relevant modules, including the (often large) number of students who worked on them before the onset of the study, and can sometimes use data from students who worked on similar modules as well. Analysis of A/B tests must discard data from these students, who were not randomized between treatment conditions and are subject to confounding. 
%Unlike lab studies, A/B tests are subject to the haphazard unpredictiability of real life, which only increases the statistical imprecision---even a sample as large as the 2,152 of \cite{harrison2020spacing} may not be enough to answer some causal questions. 

However, recent methodological innovations \cite{gagnon2021precise,sales2018using} have argued that data from the ``remnant'' from an experiment---students who were not randomized between conditions, but for whom covariate and outcome data are available---need not be discarded, but can play a valuable role in causal estimation. In fact, researchers can use data from the remnant to decrease experimental standard errors without sacrificing the unbiased estimation and design-based inference that recommend A/B testing. The basic idea is to first use the remanant data to train a machine learning model predicting outcomes as a function of covariates; then, use that fitted model to generate predicted outcomes for participants in the experiment. Finally, use those predictions as a covariate in a design-based covariate-adjusted causal estimator \cite{wu2018loop,aronowMiddleton,wager2016high,chernozhukov2018double}. Variants of the the method use the predictions from the remnant alongside other covariates to estimate causal effects.

These methods can help alleviate another weakness, shared by A/B tests and observational studies---the dependence of conclusions on statistical modeling choices. By observing outcome data prior to selecting and fitting statistical models, researchers (often inadvertantly) choose models most favorable to their desired conclusions and undermine statistical objectivity and the logic of inference. Two proposed solutions to this issue are (1) to split the sample prior to data analysis, and use one part to choose a model and the second part to estimate effects \cite{heller2009split} or (2) to rely on flexible non-parametric models that can be specified prior to data collection \cite{van2011targeted}. Design-based estimators incorporating remnant data rely on both these techniques: model-fitting in the remnant can be interative and based on human judgement, without adversely affecting the objectivity or validity of statistical inference using the experimental sample. Design-based covariate adjustment often uses robust or non-parametric models. 

This paper reviews a set of design-based causal estimators that use remnant data and then applies them to an new dataset: a collection of \Sexpr{pn(analysisResults$nExp)} multi-armed A/B tests run on the ASSISTments TestBed \cite{ostrow2016assessment}, which together include \Sexpr{analysisResults$nCont} different two-way comparisons, and \Sexpr{pn(analysisResults$nStud)} students. Alongside this experimental data, we collected log data for an additional 193,218 students who worked on similar skill builders in ASSISTments but did not participate in any of the \Sexpr{analysisResults$nExp} experiments---the remnant.
We used these datasets to estimate the causal effects of each of the tested conditions on skill-builder completion---whether students completed their assignments by demonstrating mastery on the skills in question.
Our interest here is not on the treatment effects themselves, but on the performance of remnant-based covariate adjustment--to what extent do these methods reduce estimated standard errors?
To answer that question, we compared two different estimators incorporating predictions from the remnant---one including just those predictions, and one also including other covariates---to both simple t-tests and a design-based machine learning estimator that uses only experimental data \cite{wu2018loop}.
We also compared results across estimators using four different novel deep learning models predicting outcomes as a function of covariates.

Our results give a much clearer picture of the potential impacts of using remnant data in design-based causal inference than was previously available. In particular, we find that, in typical cases, incorporating remnant data consistently improves statistical precision, sometimes substantially.
The large number of experimental contrasts also allows us to investigate the circumstances under which these methods perform best---we compare performance across sample sizes and both within and out-of sample accuracy of the deep learning algorithms. In very small samples, the behavior of the covariate adjustment methods we use can be erratic; for moderate to large samples, improvement relative to t-tests appears to decline very slightly with increasing sample sizes. Within-sample accuracy of the deep learning algorithm is closely associated with the performance of the methods, but out of sample cross-validated accuracy measures correlated weakly, at best, with precision gains.

The following section reviews design-based causal inference, covariate adjustment, and methods incorporating the remnant. Section \ref{sec:expData} describes the experimental data, Section \ref{sec:remnant} describs deep learning models that we used, along with the log data they were based on. Section \ref{sec:mainResults} describes the main results--the relative precision of the causal estimators we considered, and Section \ref{sec:explain} digs deeper, comparing precision gains across sample sizes and deep-learning model accuracy. Section \ref{sec:discussion} concludes. 

\section{Review: Design-Based Causal Inference}
For each subject $i$ in a randomized experiment, let $Z_i=1$ if $i$ is randomized to the treatment condition and $Z_i=0$ if $i$ is randomized to control, and let $Y_i$ be the outcome of interest---in the case of the A/B tests we analyze below, $Y_i=1$ if student $i$ completed the skill builder and $Y_i=0$ otherwise. 
(In the experiments we consider, it is rarely clear which condition should be labeld ``treatment'' and which should be labeled ``control''; nevertheless, we retain these familiar terms for the sake of convenience, and label contrasting conditions as treatment or control arbitrarily.)
Following \cite{neyman:1923,rubin1974estimating}, define ``potential outcomes'' $\yci$ and $\yti$ as the outcomes $i$ would have exhibited had $i$ (perhaps counterfactually) been assigned to control or treatment, respectively.
Then, assuming no spillover effects, $Y_i=Z_i\yti+(1-Z_i)\yci$---that is, if $i$ is randomized to treatment $i$'s outcome $Y_i$ is equal to $\yti$, and if $i$ is randomized to control $Y_i=\yci$.
Notably, we consider $Z_i$ to be random, but $\yci$ and $\yti$ to be fixed (however $Y_i$, which is a function of $Z_i$, is random); this is a modeling choice, motivated by the fact that the distribution of $Z_i$ is known, whereas the distributions of the potential outcomes would have to be assumed. 

The treatment effect for student $i$ is $\tau_i\equiv \yti-\yci$. 
Both potential outcomes are defined for every student in the experiment; however, only one is observed for each student.
Nevertheless, under randomization we can estimate the sample average treatment effect (ATE), $\tbar\equiv 1/n\sum_{i=1}^n\tau_i$, where $n$ is the sample size for the experiment.

Without covariates, $\tbar$ can be estimated with the ``simple difference'' or ``t-test'' estimator, which compares the sample means of $Y$ between the two treatment groups:
\begin{equation*}
  \tsd=\frac{\sum Z_iY_i}{n_T}-\frac{\sum (1-Z_i)Y_i}{n_C}
\end{equation*}
where $n_T$ and $n_C$ are the numbers of students assigned to the treatment and control conditions, respectively. The sampling variance of $\tsd$ is estimated as
\begin{equation*}
  \varhat{\tsd}=\frac{\syt}{n_T}+\frac{\syc}{n_C}
\end{equation*}
where $S^2(\cdot)$ is the sample variance function. The true sampling variance for $\tsd$, $Var\left(\tsd\right)$, depends on the covariance of $\yt$ and $\yc$, which is not identified since $\yt$ and $\yc$ are never observed simultaneously. Instead,
$\varhat{\tsd}$ is conservative in expectation \cite{imbens2015causal}---$\EE\left[\varhat{\tsd}\right]\le Var\left(\tsd\right)$.
The ratio $\tsd/\varhat{\tsd}$ is the test statistic for the unequal-variance t-test, and converges in distribution to a standard normal variate.

The approach we take to covariate adjustment, following \cite{aronowMiddleton,loop} and others, is built on an alternative estimator, which is in turn related to the Horvitz-Thompson estimator from survey sampling \cite{horvitz1952generalization}.
Let $p=Pr(Z=1)$, the probability of randomizing each subject to the treatment condition. Then estimate $\tbar$ as
\begin{equation*}
\thipw=\frac{1}{n}\displaystyle\sum_{i: Z_i=1}
\frac{Y_i}{p}-\frac{1}{n}\displaystyle\sum_{i: Z_i=0}\frac{Y_i}{1-p}
\end{equation*}
$\thipw$ is also called the ``inverse probability weighted'' or IPW estimator, due to the denominators in the expression.
In fact, in Bernoulli-randomized experiments such as those in ASSISTments, in which each subject is randomized indepdently, so $n_T$ and $n_C$ are random, the denominators of $\thipw$ are equal to the expectations of the denominators of $\tsd$, $\EE[n_T]=pn$ and $\EE[n_C]=(1-p)n$.

\subsection{Design-Based Covariate Adjustment}
The ``fundamental problem of causal inference'' \cite{holland:1986a} is that $\yti$ and $\yci$ are never observed simultaneoulsy; our ignorance of counterfactual potential outcomes is the cheif source of error in causal estimators.
It follows that the better we can anticipate a student's counterfactual potential outcome, $\yti$ if $i$ is randomized to control or $\yci$ if $i$ is randomized to treatment, the more precisely we can estimate $\tbar$.
Let $\bm{x}_i$ be a $k\times 1$ vector of baseline covariates for subject $i$, and let $\predc$ and $\predt$ be functions from $\mathbb{R}^k\rightarrow \mathbb{R}^1$ that predict $\yci$ and $\yti$, respectively, as a function of $\bm{x}_i$.
Finally, let $m_i=p\yci+(1-p)\yti$, subject $i$'s expected counterfactual potential outcome, and let $\hat{m}_i=p\predcx+(1-p)\predtx$ be it's estimate. 
Note that if $Z_i=1$, so that $Y_i=\yti$,
\begin{equation*}
  \frac{Y_i-m_i}{p}=\frac{\yti-\left(p\yci+(1-p)\yti\right)}{p}=\yti-\yci=\tau_i
\end{equation*}
so that
\begin{equation*}
  \frac{Y_i-\hat{m}_i}{p}=\frac{Y_i-m_i+m_i-\hat{m}_i}{p}=\tau_i+\frac{m_i-\hat{m}_i}{p}
\end{equation*}
and that a similar relationship holds for $(Y_i-\hat{m}_i)/(1-p)$ if $Z_i=0$.
This formalizes the notion that accuate estimation of counterfactual potential outcomes based on covariates can lead to better causal estimation. 

If $\predc$ and $\predt$ are constructed such that
\begin{equation}\label{eq:indPred}
\{\predcx,\predtx\}\independent Z_i.
\end{equation}
  then 
\begin{equation} \label{eq:thm}
\thm=
\frac{1}{n}\sum_{i \in \tg} \frac{Y_i-\hmi}{p} - 
\frac{1}{n}\sum_{i \in \cg} \frac{Y_i-\hmi}{1-p} 
\end{equation}
is an unbiased estimate for $\tbar$.
In fact, this unbiasedness holds regardless of $\predc$ or $\predt$'s other properties---they need not be unbiased, or consistent, or correct in any sense for $\thm$ to be unbiased. 

Satisfying \eqref{eq:indPred} can be a challenge, however, since $Y_i$ is a function of $Z_i$, so any model $\predc$ or $\predt$ that is fit using $Y_i$ will violate \eqref{eq:indPred}.
\cite{loop} suggests avoiding this problem with leave-one-out sample splitting. 
Specifically, for each $i=1,\dots,n$ let $\predci$ and $\predti$ be models trained on covariates and outcomes of all experimental subjects \textit{except} $i$. Then, in a Bernoulli experiment, $\hat{m}_i = p\predcxi + (1-p)\predtxi$ will satisfy \eqref{eq:indPred} since it is a function of baseline covariates $\bm{x}_i$, which are indepdent of $Z_i$ by construction, and models $\predti$ and $\predci$ which were fit to data other than $i$.
For the remainder of the paper we refer to the the estimator of \eqref{eq:thm} constructed using $\predcxi$ and $\predtxi$ as $\tss$; when we want to specify the set of covariates (e.g. $\bm{x}$) and the algorithm giving rise to $\predci$ and $\predti$, such as ordinary least squares (OLS) or random forests (RF) \cite{breiman2001random}, we will write, say $\tss[\bx,RF]$ or $\tss[x_1,OLS]$.

To estimate the sampling variance of $\tss$, define
\begin{equation} \label{Echat}
\Ech = \frac{1}{\nc}\sum_{i:Z_i=0}\left[\predcxi - \yci\right]^2; \textrm{ and } \Eth = \frac{1}{\nt}\sum_{i: Z_i=1}\left[\predtxi - \yti\right]^2
\end{equation}
the sample leave-one-out mean-squared-errors of $\predcxi$ and $\predtxi$. Then we estimate \cite{loop}
\begin{equation*}
  \hat{\var}(\tss) =  \frac{1}{n}\left[\frac{p}{1-p}\Ech + \frac{1-p}{p}\Eth  + 2\sqrt{\Ech \Eth }\right]. 
\end{equation*}
If the mean-squared error of $\predtxi$ and $\predcxi$ are low, then $\tss$ can be a highly precise estimator.

In fact, a low mean-squared error is the only relevant criterion for $\predcxi$ and $\predtxi$, since it determines $\varhat{\tss}$---correctness or unbiasedness are not necessary.

\subsection{Incorporating the Remnant}


If $\bx$ is high dimensional or has a complicated structure (or both) $\tss$ faces a dual challenge. Although high-performance machine learning algorithms are available, they often require large sample sizes to fit well. Furthermore, they often require choosing a tuning parameter or other model selection.

The remnant can be helpful here. Let $\predrfun$ be a model trained using remnant data, predicting outcomes as a function of covariates. Then, for subjects $i$ in the experiment, use the fitted model, along with covariates, to generate predictions $\predr\equiv\predrxi$.

Compared with the leave-one-out strategy, training $\predrfun$ in the remnant has both advantages and disadvantages. Often the remnant can have a much larger sample size than the experimental set, which can allow for more complex, high-dimensional, and precise modeling.
Furthermore, since the remnant is entirely independent of the experimental set, statistical inference in treatment effect estimation does not have to account for the modeling process in the remnant.
In other words, as long as only remnant data is used in training $\predrfun$, $Z_i\independent \predr$ will hold in the experimental set.
In particular, an analyst may train several models, assess their fit within the remnant, and based on the result either choose the best fitting model or formulate a new set of models and repeat the process---yet still $Z_i\independent \predr$.
This is not the case when using the experimental set to train $\predci$ and $\predti$, even when using the leave-one-out strategy of $\tss$---once $Y_i$ has been used to choose a model, even if the new model was trained using data other than $i$, its predictions will not necessarily be independent of $Z$.

On the other hand, if the distribution of $\bx$, or the joint distribution of $\bx$ and $\yt$ or $\yc$ differ between the remnant and the experimental set, a model trained in the remnant will most likely produce less accurate predictions of potential outcomes in the experiment than one trained using experimental data.
In fact, \cite{sales2018using} included an example in which a model fit in the remnant produced predictions in the experiment that were \textit{anti}correlated with outcomes, and using the method of that paper, substantially increased standard errors.

Ideally, a method for causal estimation incorporating $\predr=\predxr$ will make optimal use of it when it has a low mean-squared-error in the experimental set, but discard or downweight it when it is a poor predictor. 
\cite{gagnon2021precise} suggests several options for incorporating $\predri$ into causal estimates, but in this paper we focus on two, both of which make use of the $\tss$ family of estimator.
The first is $\tss[\predr;OLS]$. This estimator uses leave-one-out to generate predictions of the form
\begin{equation*}
  \predci=\hat{\beta}_{-i0}+\hat{\beta}_{-i1}\predr
\end{equation*}
with an analogous expression for $\predti$.
The hope is that if $\predr$ is, on average, an accurate prediction of $\yt$ or $\yc$, $\hat{\beta}_0\approx 1$ and $\predr$ will take the place of $\predc$ or $\predt$ in \eqref{eq:thm}.
If $\predr$ is, on average, a poor predictor of potential outcomes, then $\hat{\beta}_0\approx 0$.

$\tss[\predr;OLS]$ captures the advantages of using the remnant to train prediction models, but if the joint distribution of $Y$ and $\xb$ differs widely from the remant to the experimental set, $\predr$ is unlikely to be a good predictor of $Y$ and incorporating other covariates may improve performance.
That motivates the other estimator we will consider, which balances between $\tss[(\xb,\predr);RF]$--- that is, $\tss$ using the random forest prediction algorithm and the full set of covariates $\xb$, along with $\predr$---and $\tss[\predr,OLS]$, which may be optimal if $\predr$ has a low mean squared error.
This ensemble estimator, which we will refer to as $\tss[\xt;EN]$ uses both within-RCT covariate adjustment as well as remnant-based adjustment, exploiting the advantages of both.


\section{Data from \Sexpr{analysisResults$nExp} Experiments}

The ASSISTments TestBed is a platform that allows researchers to design educational experiments that will then be run within the ASSISTments online tutor. Education researchers can specify experimental conditions, including variation on how subject matter is portrayed, available hints, and feedback to students. Researchers also choose learning modules on which their experiments run.
When teachers subsequently assign these modules to their students, the students are randomized between the conditions.
After the period of the experiment has ended, the researcher is provided with a dataset, including classroom and student identifiers, log data from during the experiment, and outcome data such as which students completed the assignment and how many problems they worked.
Students are randomized between conditions indepdently, one at a time; when there are only two conditions, this is Bernoulli randomization.

We gathered a set of \Sexpr{analysisResults$nExp} A/B tests run on the TestBed. Since our interest here is primarily methodological, with the goal of reducing standard errors, we focus on estimated standard errors as opposed to treatment effects. Our analyses will focus on completion as a binary outcome.

We also gathered a set of student-level aggregated predictors, to be used for within-RCT covariate adjustment. These were the numbers of skill builders and problems sets each student began and completed, as welll as each student's prior median first response time when working ASSISTments problems, median time on task, overall correctness, and average attempt count.   

Several experiments included multiple conditions, rather than only treatment and control. We assume that primary interest in these experiments focuses on head-to-head comparisons between conditions, and, as such, we analyze all unique pairs of conditions within randomized experiments separately.
All in all, this includes \Sexpr{analysisResults$nContTot} pairs.
However, not every pair was amenable to analysis. Six pairwise contrasts were dropped because the outcome variance in one or both of the conditions was zero.
Further exclusions recommended themselves, though perhaps more controversially. To maintain objectivity and rigor, below we present analysis using all remaing \Sexpr{analysisResults$nCont} pairwise comparisons, as well as analysis using only the \Sexpr{analysisResults$nContSmall} that remained after the most aggressive data exclusion we considered.

Further exclusions were motivated by two factors: first, the $\tss$ estimator presumes that $p=P(Z_i)$ is known. In theory, $p=1/2$ should hold in all pairwise comparisons. However, there were strong indications that that some subsets of experiments used a different randomization scheme that we did not have access to.
Instead, we estimated p-values testin the null hypothesis that $p=1/2$ for each comparison we considered; in the full data analysis we included all contrasts, but in the exclusionary analysis we dropped contrasts in which the p-value testing $p=1/2$ was $>0.1$.
Secondly, there were some contrasts which included extremely small samples, with the smallest being $n=16$.
The $\tss$ estimators rely on OLS regression or more complex models, and cannot be expected to perform well when sample sizes are so small.
In the exclusionary analysis, we dropped experiments in which the sample size in either condition was less than $5(k+2)+1$, where $k=9$ is the number of predictos, which would allow for at least 5 observations per predictor in any model.


\section{Data Collection}

Within ASSISTments, researchers can create experimental skill builders through the ASSISTments TestBed \cite{selent2016assistments}. Skill builders are mathematics problem sets in which students are given a series of similar problems on the same group of mathematics skills until they demonstrate mastery by getting three problems correct in a row. Experimental skill builders randomize students between different conditions within a skill builder. In these conditions students may be given different problem content, different tutoring when they get the problem incorrect, different mastery requirements, different choices in what problems they complete, and more. For example, Figure \ref{fig:tutor} shows two conditions in an experimental skill builder, one in which the students receive a text-based explanation to an example problem, and the other in which the students receive a video explanation to an example problem.

\begin{figure}
\includegraphics[width=\textwidth]{tutor.png}
\caption{Two conditions in an experimental skill builder. On the left, students receive video when they ask for support. On the right, students receive text when they ask for support.}
\label{fig:tutor}
\end{figure}

The data was collected from ASSISTments in two sets, remnant data, and experiment data. Remnant data was used to train the imputation models, and experiment data was used to determine the outcomes of each experiment using the imputation models and RELOOP. The skill builders started by the students in the remnant data were not the same skill builders as the experimental skill builders in the experiment data, nor is there any overlap in students between the two datasets. \textbf{No information from the students or skill builders in the experiment data was in the remnant data used to train the imputation models}.

For both the remnant and experiment data, the same information was collected. For each instance of a student starting a skill builder for the first time, data on whether they completed the skill builder, and if so, how many problems they had to complete before mastering the material was collected. The imputation models, discussed more in section \ref{sec:imputation} were trained to predict these two dependent measures. The data used to predict these dependent measures was aggregated from all of the previous work done by the student. Three different sets of data were collected for each sample in the datasets: prior student statistics, prior assignment statistics, and prior daily actions. Prior student statistics included the past performance of each student, for example, their prior percent correct, prior time on task, and prior assignment completion percentage. Prior assignment statistics were aggregated for each assignment the student started prior to the skill builder. Prior assignment statistics included things like the skill builders' unique identifier (or in the remnant data, the ID of the experimental version of a skill builder, if it existed), how many problems had to be completed in the assignment, students' percent correct on the assignment, and how many separate sessions students' used to complete the assignment. Prior daily actions contained the total number of times students performed each possible action in the ASSISTments Tutor for each day prior to the day they started the skill builder. The possible actions included things like starting a problem, completing an assignment, answering a problem, and requesting support. 193,218 sets of prior statistics on students, 837,409 sets of statistics on prior assignments, and 695,869 days of students' actions were aggregated for the remnant data, and 113,963 sets of prior statistics on students, 2,663,421 sets of statistics on prior assignments, and 926,486 days of students' actions were aggregated for the experiment data. The full dataset used in this work can be found at https://osf.io/k8ph9/?view\_only=ca7495965ba047e5a9a478aaf4f3779e.

\section{Imputation Models}\label{sec:imputation}

\section{Model Design}

Each of the three types of data in the remnant dataset were used to predict both skill builder completion and number of problems completed for mastery. For each type of data: prior student statistics, prior assignment statistics, and prior daily actions, a separate neural network was trained. Additionally, a fourth neural network was trained using a combination of the previous three models. The prior student statistics model, shown in Figure \ref{fig:models} in red was a simple feed forward network with a single hidden layer of nodes using sigmoid activation and dropout. Both the prior assignment statistics model and the prior daily actions model, shown in Figure \ref{fig:models} in blue and yellow respectively, were recurrent neural networks with a single hidden layer of LSTM nodes \cite{gers2000learning} with both layer-to-layer and recurrent dropout. The prior assignment statistics model used the last 20 started assignments as input, and the prior daily actions model used the last 60 days of actions as input. The combined model in Figure \ref{fig:models} takes the three models above and couples their predictions, such that the prediction is a function of all three models weights and the loss same loss is backpropigated through each model during training.

\begin{figure}
\includegraphics[width=\textwidth]{model.pdf}
\caption{All four of the imputation models in one. The red model predicts performance using only prior statistics of the student, the blue model uses statistics on the last 20 assignments completed by the student to predict performance, and the yellow model uses the last 60 days of actions the student took in the tutor. The combined model, shown in grey, uses all three models to predict performance.}
\label{fig:models}
\end{figure}

\subsection{Model Training}

To select the best model hyperparameters and to measure the quality of each imputation model, 5-fold cross validation was used to train and calculate various metrics for each model. For all training, the ADAM method \cite{kingma2014adam} was used during backpropigation, binary cross-entropy loss was used for predicting completion, and mean squared error loss was used for problems to mastery. The total loss for each model was the sum of the two individual losses. Because mean squared error and binary cross-entropy have different scales, a gain of 16 was applied to the binary cross-entropy loss, which brought the loss into the same range as the mean squared error loss for this particular dataset. Table \ref{tab:training} shows various metrics of the models' quality. Interestingly, even though all the models are bad at predicting problems to mastery, removing problems to mastery from the loss function reduced the models ability to predict completion.

\begin{table}
\caption{Metrics Calculated from 5-Fold Cross Validation for each Model}
\begin{tabular}{|r|c|c|c|c|}
\hline
       & Prior Student    & Prior Assignment & Prior Daily   & \\
Metric & Statistics       & Statistics       & Action Counts & Combined \\
\hline
Completion AUC       & 0.743 & 0.755 & 0.658 & \textbf{0.770} \\
Completion Accuracy  & 0.761 & 0.767 & 0.743 & \textbf{0.774} \\
Completion $r^2$     & 0.143 & 0.161 & 0.045 & \textbf{0.184} \\
\# of Problems MSE   & 8.489 & 8.505 & 8.719 & \textbf{8.363} \\
\# of Problems $r^2$ & 0.033 & 0.032 & 0.007 & \textbf{0.048} \\
\hline
\end{tabular}
\label{tab:training}
\end{table}

Based on Table \ref{tab:training}, statistics on prior assignments was the most predictive of students' assignment performance, followed by the students' overall prior performance statistics, and then their daily action history, which was the least predictive of their performance on their next assignment. Combining these datasets together led to predictions of a higher quality than any individual dataset could achieve.

\begin{table}
\caption{Prior Student Statistics Features}
\begin{tabular}{|r|l|}
\hline
Name & Description \\
\hline
target\_sequence & The ID of the experimental skill builder \\
has\_due\_date & Whether the skill builder had a due date \\
student\_prior\_assignments\_started & The number of assignments previously started by the student \\
student\_prior\_assignments\_percent\_completed & The number of assignments previously completed by the student \\
student\_prior\_median\_ln\_assignment\_time\_on\_task & The median of the log of the time between starting and finishing an assignment for all the students completed prior assignments \\
student\_prior\_average\_problems\_per\_assignment & The average number of problems completed by the student across all their previous assignments \\
student\_prior\_median\_ln\_problem\_time\_on\_task & The median of the log of the time the student took between starting and finished all their completed prior problems \\
student\_prior\_median\_ln\_problem\_first\_response\_time & The median of the log of the time the student took to submit their first answer or request tutoring across all their completed prior problems \\
student\_prior\_average\_problem\_correctness & The fraction of previously completed problems the student got correct on their first attempt without tutoring \\
student\_prior\_average\_problem\_attempt\_count & The average number of attempts for all problems previously completed by the student \\
student\_prior\_average\_answer\_first & The fraction of times the student submitted an answer before requesting tutoring for all problems previously completed by the student \\
student\_prior\_average\_problem\_hint\_count & The average number of hints requested for all problems previously completed by the student \\
\hline
All Following Features & These features are the same as the features above with a similar name, but only calculate statistics across problems with the same skills as the problems in the experimental skill builder \\
\hline
student\_skill\_prior\_average\_problems\_per\_assignment \\
student\_skill\_prior\_median\_ln\_problem\_time\_on\_task \\
student\_skill\_prior\_median\_ln\_problem\_first\_response\_time \\
student\_skill\_prior\_average\_problem\_correctness \\
student\_skill\_prior\_average\_problem\_attempt\_count \\
student\_skill\_prior\_average\_answer\_first \\
student\_skill\_prior\_average\_problem\_hint\_count \\
\hline
\end{tabular}
\label{tab:pssf}
\end{table}

\begin{table}
\caption{Prior Assignment Statistics Features}
\begin{tabular}{|r|l|}
\hline
Name & Description \\
\hline
student\_id & The ID of the student \\
assignment\_start\_time & The UNIX time of when the assignment was started \\
directory\_1 & The highest level directory of the assignment location, usually an indication of curriculum \\
directory\_2 & The second level directory of the assignment location, usually an indication of grade level \\
directory\_3 & The third level directory of the assignment location, usually an indication of unit \\
sequence\_id & The unique ID of the skill builder assignment, or the corresponding normal skill builder ID for experiments \\
is\_skill\_builder & Boolean flag for whether or not this assignment is a skill builder or a normal problem set \\
has\_due\_date & Boolean flag for if the assignment has a due date \\
assignment\_completed & Boolean flag for if the student completed the assignment \\
time\_since\_last\_assignment\_start & The time between the student starting this assignment and starting their prior assignment \\
\hline
All Following Features & In addition to the raw value, a value z-scored across all students who completed the assignment previously, and a percentile across students in the same class who completed the assignment previously was included in the model as well. \\
\hline
session\_count & How many times the student left and rejoined the assignment \\
day\_count & How many days the student worked on the assignment for \\
completed\_problem\_count & How many problems the student completed in the assignment \\
median\_ln\_problem\_time\_on\_task & The median of the log of the time between the student starting and finishing problems in the assignment \\
median\_ln\_problem\_first\_response & The median of the log of the time it took for the student to submit their first answer or request tutoring on the problems they started in the assignment \\
average\_problem\_attempt\_count & The average number of attempts the student made on the problems in the assignment \\
average\_problem\_answer\_first & The fraction of times the student made an attempt before requesting tutoring on all the problems in the assignment \\
average\_problem\_correctness & The fraction of times the student got the problem correct on their first try on all the problems in the assignment \\
average\_problem\_hint\_count & The average number of hints used by the student on all the problems in the assignment \\
average\_problem\_answer\_given & The fraction of times the student was given the answer on all the problems in the assignment \\

\hline
\end{tabular}
\label{tab:pasf}
\end{table}

\begin{table}
\caption{Prior Daily Actions Features}
\begin{tabular}{|r|l|}
\hline
Name & Description \\
\hline
student\_id & The ID of the student \\
timestamp & The UNIX time at 00:00:00 of the day the action counts apply to \\
ln\_action\_1\_count & Log of the count of assignment started actions taken \\
ln\_action\_2\_count & Log of the count of assignment resumed actions taken \\
ln\_action\_3\_count & Log of the count of assignment finished actions taken \\
ln\_action\_4\_count & Log of the count of problem set started actions taken \\
ln\_action\_5\_count & Log of the count of problem set resumed actions taken \\
ln\_action\_6\_count & Log of the count of problem set finished actions taken \\
ln\_action\_7\_count & Log of the count of problem set mastered actions taken \\
ln\_action\_8\_count & Log of the count of problem set exhausted actions taken \\
ln\_action\_9\_count & Log of the count of problem limit exceeded actions taken \\
ln\_action\_10\_count & Log of the count of problem started actions taken \\
ln\_action\_11\_count & Log of the count of problem resumed actions taken \\
ln\_action\_12\_count & Log of the count of problem finished actions taken \\
ln\_action\_13\_count & Log of the count of tutoring set started actions taken \\
ln\_action\_15\_count & Log of the count of tutoring set finished actions taken \\
ln\_action\_16\_count & Log of the count of hint requested actions taken \\
ln\_action\_17\_count & Log of the count of scaffolding requested actions taken \\
ln\_action\_19\_count & Log of the count of explanation requested actions taken \\
ln\_action\_20a\_count & Log of the count of student correct response actions taken \\
ln\_action\_20b\_count & Log of the count of student incorrect response actions taken \\
ln\_action\_21\_count & Log of the count of open response submission actions taken \\
ln\_action\_25\_count & Log of the count of answer requested actions taken \\
ln\_action\_26\_count & Log of the count of continue selected actions taken \\
ln\_action\_30\_count & Log of the count of help requested actions taken \\
ln\_action\_31\_count & Log of the count of timer started actions taken \\
ln\_action\_32\_count & Log of the count of timer resumed actions taken \\
ln\_action\_33\_count & Log of the count of timer paused actions taken \\
ln\_action\_34\_count & Log of the count of timer finished actions taken \\
ln\_action\_35\_count & Log of the count of live tutoring requested actions taken \\
Other Actions & Artifacts of the database, always 0 \\
\hline
\end{tabular}
\label{tab:pdaf}
\end{table}

\section{Results: Comparing Standard Errors across A/B Tests, Models, and Methods}
\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{figure/fig3-1.pdf}
\caption{Boxplots and jittered scatter plots of the ratios of estimated sampling variances of $\tsd$, $\tss[\predr;OLS]$, $\tss[\xb;RF]$, and $\tss[\xt;EN]$}
\label{fullRes}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{figure/fig4-1.pdf}
\caption{Boxplots and jittered scatter plots of the ratios of estimated sampling variances of $\tsd$, $\tss[\predr;OLS]$, $\tss[\xb;RF]$, and $\tss[\xt;EN]$ on restricted data.}
\label{restRes}
\end{figure}

We compare the sampling variances between methods by taking the ratios of the estimated sampling variances.
Since sampling variance scales as $1/n$, ratios of sampling variances can be thought of as "sample size multipliers"---that is, decreasing the variance by a factor of $q$ is analogous to increasing the sample size by the same factor. 

Figure \ref{fullRes} shows boxplots of sampling variance ratios, along with jittered individual points. Each point represents a particular randomized comparison, analyzed using a particular model fit in the remnant. 
The panel on the left compares $\tss[\predr;OLS]$ to $\tsd$, the t-test estimator. In nearly every case the estimator using remnant date substantially outperformed the t-test estimator. In the majority of cases, including remnant-based predictions was roughly equivalent to increasing the sample by between 10 and 50\% but in some rare cases the improvement was of a factor of five or more. In a handful of cases $\tss[\predr;OLS]$ performed substantially worse---further investigation revealed the cases to be experiments with extremely small sample sizes, in which OLS cannot be expected to perform well.
Comparing across models fit in the remnant, the action-level model performed the worst, while the combined model was responsible for the greatest decrease in sampling variance.
Interestingly, the assignment-level model performed nearly as well as the combined model, suggesting that action- and student-level data did not contribute substantially.

The middle panel of Figure \ref{fullRes} compares $\tsd$ to $\tss[\xt;EN]$. Here the results are nearly identical to those of the left panel, suggesting that combining within-RCT covariate adjustment with remnant-based predictions did not significantly enhance performance. 

The rightmost panel of Figure \ref{fullRes} compares $\tss[\xb;RF]$, which uses leave-one-out sample splitting and a random forest to adjust for covariates---but does not use the remnant---to $\tss[\xt;EN]$ which does. In this case we see more modest gains, which is to be expected, since $\tss[\xb;RF]$ can accomplish a good deal of covariate adjustment using only experimental data. 
Nevertheless, the contribution of the remnant is still significant---in a large number of cases, including data from the remnant was equivalent to increasing the sample size by 10 or 20\%.

Figure \ref{restRes} reports similar results as Figure \ref{fullRes}, but it excludes cases in which the p-value testing the hypothesis that $Pr(Z=1)=0.5$ was less than $0.1$, or in which there were fewer than 56 cases in each treatment arm. 
The number 56 was chosen so that if we include 10 covariates in addition to the remnant-based predictions, and leave one case out, there will be sufficient sample size for five cases per predictor, a common rule of thumb for regression modeling. 
Removing comparisons with small or imbalanced samples greatly reduces the range of outcomes, completely eliminating the extreme outliers of Figure \ref{fullRes}.
Now, there are no cases in which including remnant-based predictions severely increases standard errors. 
At the same time, the extreme gains, on the order of 500\%, are no longer represented.
Focusing on results for the combined model, which continues to perform the best, including remnant-based predictions is equivalent to increasing the sample size, relative to a t-test, by a factor of about 10-25\% in about half of all cases, but up to 50\%-70\% in the most extreme cases. 
Compared to $\tss[\xb;RF]$, including remnant based predictions was equivalent to increasing the sample size by roughly 8-12\% in half of all cases, but as much as 30\% in others. 

\section{Explaing Performance}



\subsubsection{Acknowledgements} Please place your acknowledgments at
the end of the paper, preceded by an unnumbered run-in heading (i.e.
3rd-level heading).

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
 \bibliographystyle{splncs04}
 \bibliography{bib,rebarloop,rebarloop2}
%

\end{document}
