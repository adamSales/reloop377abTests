\section{Data Collection}

Within ASSISTments, researchers can create experimental skill builders through the ASSISTments TestBed \cite{selent2016assistments}. Skill builders are mathematics problem sets in which students are given a series of similar problems on the same group of mathematics skills until they demonstrate mastery by getting three problems correct in a row. Experimental skill builders randomize students between different conditions within a skill builder. In these conditions students may be given different problem content, different tutoring when they get the problem incorrect, different mastery requirements, different choices in what problems they complete, and more. For example, Figure \ref{fig:tutor} shows two conditions in an experimental skill builder, one in which the students receive a text-based explanation to an example problem, and the other in which the students receive a video explanation to an example problem.

\begin{figure}
\includegraphics[width=\textwidth]{tutor.png}
\caption{Two conditions in an experimental skill builder. On the left, students receive video when they ask for support. On the right, students receive text when they ask for support.}
\label{fig:tutor}
\end{figure}

The data was collected from ASSISTments in two sets: remnant data and experiment data. Remnant data was used to train the imputation models, and experiment data was used to determine the outcomes of each experiment using the imputation models and RELOOP. The skill builders started by the students in the remnant data were not the same skill builders as the experimental skill builders in the experiment data, nor is there any overlap in students between the two datasets. \textbf{No information from the students or skill builders in the experiment data was in the remnant data used to train the imputation models}.

For both the remnant and experiment data, the same information was collected. For each instance of a student starting a skill builder for the first time, data on whether they completed the skill builder, and if so, how many problems they had to complete before mastering the material was collected. The imputation models, discussed more in section \ref{sec:imputation} were trained to predict these two dependent measures. The data used to predict these dependent measures was aggregated from all of the previous work done by the student. Three different sets of data were collected for each sample in the datasets: prior student statistics, prior assignment statistics, and prior daily actions. Prior student statistics included the past performance of each student, for example, their prior percent correct, prior time on task, and prior assignment completion percentage. Prior assignment statistics were aggregated for each assignment the student started prior to the skill builder. Prior assignment statistics included things like the skill builders' unique identifier (or in the  experimental data, the ID of the non-experimental version of that skill builder, if it existed), how many problems had to be completed in the assignment, students' percent correct on the assignment, and how many separate sessions students' used to complete the assignment. Prior daily actions contained the total number of times students performed each possible action in the ASSISTments Tutor for each day prior to the day they started the skill builder. The possible actions included things like starting a problem, completing an assignment, answering a problem, and requesting support. 193,218 sets of prior statistics on students, 837,409 sets of statistics on prior assignments, and 695,869 days of students' actions were aggregated for the remnant data, and 113,963 sets of prior statistics on students, 2,663,421 sets of statistics on prior assignments, and 926,486 days of students' actions were aggregated for the experiment data. The full dataset used in this work can be found at https://osf.io/k8ph9/?view_only=ca7495965ba047e5a9a478aaf4f3779e.

\section{Imputation Models}\label{sec:imputation}

\section{Model Design}

Each of the three types of data in the remnant dataset were used to predict both skill builder completion and number of problems completed for mastery. For each type of data: prior student statistics, prior assignment statistics, and prior daily actions, a separate neural network was trained. Additionally, a fourth neural network was trained using a combination of the previous three models. The prior student statistics model, shown in Figure \ref{fig:models} in red was a simple feed forward network with a single hidden layer of nodes using sigmoid activation and dropout. Both the prior assignment statistics model and the prior daily actions model, shown in Figure \ref{fig:models} in blue and yellow respectively, were recurrent neural networks with a single hidden layer of LSTM nodes \cite{gers2000learning} with both layer-to-layer and recurrent dropout. The prior assignment statistics model used the last 20 started assignments as input, and the prior daily actions model used the last 60 days of actions as input. The combined model in Figure \ref{fig:models} takes the three models above and couples their predictions, such that the prediction is a function of all three models weights and the loss same loss is backpropigated through each model during training.

\begin{figure}
\includegraphics[width=\textwidth]{model.pdf}
\caption{All four of the imputation models in one. The red model predicts performance using only prior statistics of the student, the blue model uses statistics on the last 20 assignments completed by the student to predict performance, and the yellow model uses the last 60 days of actions the student took in the tutor. The combined model, shown in grey, uses all three models to predict performance.}
\label{fig:models}
\end{figure}

\subsection{Model Training}

To select the best model hyperparameters and to measure the quality of each imputation model, 5-fold cross validation was used to train and calculate various metrics for each model. For all training, the ADAM method \cite{kingma2014adam} was used during backpropigation, binary cross-entropy loss was used for predicting completion, and mean squared error loss was used for problems to mastery. The total loss for each model was the sum of the two individual losses. Because mean squared error and binary cross-entropy have different scales, a gain of 16 was applied to the binary cross-entropy loss, which brought the loss into the same range as the mean squared error loss for this particular dataset. Table \ref{tab:training} shows various metrics of the models' quality. Interestingly, even though all the models are bad at predicting problems to mastery, removing problems to mastery from the loss function reduced the models ability to predict completion.

\begin{table}
\caption{Metrics Calculated from 5-Fold Cross Validation for each Model}
\begin{tabular}{|r|c|c|c|c|}
\hline
       & Prior Student    & Prior Assignment & Prior Daily   & \\
Metric & Statistics       & Statistics       & Action Counts & Combined \\
\hline
Completion AUC       & 0.743 & 0.755 & 0.658 & \textbf{0.770} \\
Completion Accuracy  & 0.761 & 0.767 & 0.743 & \textbf{0.774} \\
Completion $r^2$     & 0.143 & 0.161 & 0.045 & \textbf{0.184} \\
\# of Problems MSE   & 8.489 & 8.505 & 8.719 & \textbf{8.363} \\
\# of Problems $r^2$ & 0.033 & 0.032 & 0.007 & \textbf{0.048} \\
\hline
\end{tabular}
\label{tab:training}
\end{table}

Based on Table \ref{tab:training}, statistics on prior assignments was the most predictive of students' assignment performance, followed by the students' overall prior performance statistics, and then their daily action history, which was the least predictive of their performance on their next assignment. Combining these datasets together led to predictions of a higher quality than any individual dataset could achieve.